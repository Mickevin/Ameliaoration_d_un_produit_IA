{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mickevin/Ameliaoration_d_un_produit_IA/blob/main/7_Multi_Layer_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Oxw6NesczL"
      },
      "source": [
        "# Multi Layer Perceptron (MLP)\n",
        "\n",
        "\n",
        "# 1. Réseaux de neurones artificiels\n",
        "\n",
        "\n",
        "Un Multi Layer Perceptron (MLP) est un réseau de neurones artificiels composé de plusieurs couches de perceptron, où chaque neurone d'une couche est connecté à tous les neurones de la couche suivante. Les couches intermédiaires entre la couche d'entrée et la couche de sortie sont appelées couches cachées.\n",
        "\n",
        "Chaque neurone d'un MLP utilise une fonction d'activation non linéaire pour transformer la somme pondérée des signaux d'entrée en une sortie. Les poids et les biais de chaque neurone sont appris par rétropropagation (backpropagation) lors de l'entraînement du réseau à partir d'un ensemble de données d'entraînement.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/ECE-Engineer/TensorFlow-Multilayer-Perceptron/master/MLP.png'>\n",
        "\n",
        "Un réseau de neuronnes est  caractérisé par : \n",
        "- le nombre de couches qu'il contient, \n",
        "- le nombre de neurones par couche,\n",
        "- la fonction d'activation utilisée par chaque neurone.\n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/TensorFlow_logo.svg/1200px-TensorFlow_logo.svg.png' width=200>\n",
        "\n",
        "[TensorFlow](https://www.tensorflow.org/) est une bibliothèque open source de calcul numérique et de machine learning, développée par Google Brain Team. Elle permet de créer et d'entraîner des de deeplearning  pour diverses tâches telles que la classification, la reconnaissance d'image, la détection d'objet, la segmentation d'image, la génération de texte et bien plus encore. \n",
        "\n",
        "Les modèles de deeplearning sont créés en assemblant des couches de neurones et en ajustant les poids de ces neurones lors de l'entraînement pour améliorer les performances du modèle. TensorFlow est utilisé dans de nombreuses industries et domaines, tels que la reconnaissance vocale, la robotique, la vision par ordinateur, la biologie, la finance, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRVVJrK5sczN",
        "outputId": "ea34ea44-03a2-4e9b-ca7c-4135f036a158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metal device set to: Apple M1 Max\n",
            "\n",
            "systemMemory: 32.00 GB\n",
            "maxCacheSize: 10.67 GB\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 6)                 66        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 21        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 6)                 24        \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 7         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 118\n",
            "Trainable params: 118\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-07 11:35:06.634610: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2023-04-07 11:35:06.634742: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MultiLayerPerceptron:\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "\n",
        "        # On crée un modèle séquentiel\n",
        "        self.model = tf.keras.models.Sequential() \n",
        "\n",
        "        # Première couche cachée de 3 neurones\n",
        "        self.model.add(tf.keras.layers.Dense(6, input_dim=input_dim, activation='sigmoid'))\n",
        "\n",
        "        # Deuxième couche cachée de 3 neurones\n",
        "        self.model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "        self.model.add(tf.keras.layers.Dense(6, activation='sigmoid'))\n",
        "\n",
        "        # Couche de sortie de 2 neurones\n",
        "        self.model.add(tf.keras.layers.Dense(output_dim, activation='softmax'))\n",
        "\n",
        "    \n",
        "model = MultiLayerPerceptron(10, 1).model\n",
        "\n",
        "\n",
        "model.compile(optimizer='sgd', loss='mean_absolute_error', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdQ9Ia_9sczO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m47DlVq0sczO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qmjQ0KJsczO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf0Mg9q4sczO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzzsewZ8sczO"
      },
      "outputs": [],
      "source": [
        "#Séparation des données en données d'entrainement et de test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WQt-1fgsczP",
        "outputId": "b518f638-7730-4538-ca93-188b41e64566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "   1/1030 [..............................] - ETA: 5:23 - loss: 0.4000 - accuracy: 0.6000"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-07 11:26:10.360590: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1030/1030 [==============================] - ETA: 0s - loss: 0.6884 - accuracy: 0.3116"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-04-07 11:26:17.492447: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1030/1030 [==============================] - 8s 8ms/step - loss: 0.6884 - accuracy: 0.3116 - val_loss: 0.7051 - val_accuracy: 0.2949\n",
            "Epoch 2/2\n",
            "1030/1030 [==============================] - 9s 8ms/step - loss: 0.6884 - accuracy: 0.3116 - val_loss: 0.7051 - val_accuracy: 0.2949\n"
          ]
        }
      ],
      "source": [
        "# Entrainement du modèle\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test),epochs=5, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7GzynyVsczP",
        "outputId": "a7891f06-d255-41ba-9133-a8e7784150e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "258/258 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.7051\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.2948746085166931, 0.7051281929016113]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Evaluation du modèle\n",
        "model.evaluate(X_test, y_test, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg_jLR3rsczP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6CLNWc7sczP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aepNvYHsczP"
      },
      "source": [
        "## 2. Fonction d'activation\n",
        "\n",
        "Une fonction d'activation est une fonction mathématique appliquée à la sortie d'un neurone dans un réseau de neurones artificiels. Elle introduit de la non-linéarité dans les sorties des neurones et permet ainsi au réseau de neurones de modéliser des relations complexes entre les entrées et les sorties.\n",
        "\n",
        "Il existe plusieurs types de fonctions d'activation, chacune avec ses propres avantages et inconvénients. Voici quelques exemples de fonctions d'activation couramment utilisées :\n",
        "\n",
        "- [Fonction sigmoïde](https://www.tensorflow.org/api_docs/python/tf/keras/activations/sigmoid) : une fonction en forme de S qui comprime les valeurs d'entrée entre 0 et 1, souvent utilisée pour la classification binaire ou la régression logistique.\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/SigmoidFunction.svg/2560px-SigmoidFunction.svg.png' width=300>\n",
        "\n",
        "- [Fonction tangente hyperbolique](https://www.tensorflow.org/api_docs/python/tf/keras/activations/tanh) (tanh) : une fonction similaire à la fonction sigmoïde, mais qui comprime les valeurs d'entrée entre -1 et 1.\n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/langfr-2880px-Hyperbolic_Tangent.svg.png' width=400>\n",
        "\n",
        "- [Fonction ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/activations/relu) (Rectified Linear Unit) : une fonction qui renvoie la valeur d'entrée si elle est positive, sinon renvoie 0. Cette fonction est souvent utilisée pour les réseaux de neurones profonds en raison de sa simplicité et de son efficacité.\n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/2880px-Activation_rectified_linear.svg.png' width=400>\n",
        "\n",
        "- [Fonction softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) : une fonction qui renvoie un vecteur de probabilités normalisées, souvent utilisée pour la classification multi-classes.\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:480/1*5nKWsukS6lPR-7fHtlK2Rg.png'>\n",
        "\n",
        "Le choix de la fonction d'activation dépend du type de problème de modélisation, du type de données en entrée et de la structure du réseau de neurones.\n",
        "\n",
        "D'autres fonctions d'activation existent et peuvent être utilisées dans TensorFlow. Vous pouvez consulter la [documentation TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/activations) pour plus d'informations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CChHAIEzsczQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxZYR-eYsczQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5p9-_GHsczQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHVVKzJgsczQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN6aBHmnsczQ"
      },
      "source": [
        "## 2. Fonction de coût et descente de gradient\n",
        "\n",
        "<img src='https://i.stack.imgur.com/VIeBL.png' width=700>\n",
        "\n",
        "### Fonction de coût\n",
        "Une fonction de perte (ou fonction d'erreur) est une mesure utilisée pour évaluer la qualité des prédictions d'un modèle de machine learning. Elle calcule l'écart entre les sorties prédites par le modèle et les sorties attendues (étiquettes de classe) pour un ensemble de données d'entraînement. \n",
        "\n",
        "L'objectif de l'apprentissage automatique est de minimiser cette fonction de perte, c'est-à-dire de trouver les paramètres du modèle qui minimisent l'écart entre les prédictions et les étiquettes de classe réelles.\n",
        "\n",
        "La fonction de perte est spécifique au type de tâche de classification ou de régression que le modèle tente de résoudre :\n",
        "\n",
        "\n",
        "**=> Pour de la classification binaire on retrouve :**\n",
        "- l'entropie croisée binaire, Elle mesure l'écart entre la probabilité prédite de la classe positive et la valeur réelle de la classe. loss = mean(- y_true * log(y_pred) - (1 - y_true) * log(1 - y_pred))\n",
        "\n",
        "`loss='binary_crossentropy'`\n",
        "\n",
        "- la fonction de perte logistique, qui est similaire à l'entropie croisée binaire, mais qui est plus stable pour les valeurs de probabilité proches de 0 ou 1.\n",
        "\n",
        "`loss='log_loss'`\n",
        "\n",
        "**=> Pour la classification multiclasse, on utilise souvent :**\n",
        "- l'entropie croisée catégorielle, elle mesure la distance entre la distribution de probabilité de sortie du modèle et la distribution de probabilité des étiquettes réelles. loss = mean(- sum(y_true * log(y_pred)))\n",
        "\n",
        "`loss='categorical_crossentropy'`\n",
        "\n",
        "**=> Pour de la regression, on retrouve :**\n",
        "- Erreur quadratique moyenne (Mean Squared Error), elle est calculée en prenant la moyenne des carrés des différences entre les valeurs de sortie prédites et les valeurs réelles. mean(mean((y_true - y_pred) ** 2))\n",
        "\n",
        "`loss='mean_squared_error'`\n",
        "\n",
        "- Erreur absolue moyenne (Mean Absolute Error), elle mesure la moyenne des écarts absolus entre les valeurs prédites et les valeurs réelles. loss = mean(mean(abs(y_true - y_pred)))\n",
        "\n",
        "`loss='mean_absolute_error'`\n",
        "\n",
        "En plus de la fonction de perte, il existe également des fonctions de régularisation qui aident à prévenir le surapprentissage en ajoutant des pénalités à la fonction de perte pour les paramètres du modèle qui sont trop grands.\n",
        "\n",
        "D'autre fonction de pertes existent et peuvent être utilisées dans TensorFlow. Vous pouvez consulter la [documentation TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses) pour plus d'informations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXyMsuFfsczR"
      },
      "source": [
        "### Descente de gradient\n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/6/68/Gradient_ascent_%28surface%29.png'>\n",
        "\n",
        "La descente de gradient est un algorithme d'optimisation qui agit sur les poids et les biais (ou \"biais de neuronnes\") du réseau de neurones.\n",
        "\n",
        "L'objectif de la descente de gradient est de minimiser la fonction de perte du modèle en ajustant les poids et les biais du réseau de neurones. \n",
        "\n",
        "Pour ce faire, elle calcule le gradient de la fonction de perte par rapport à chaque poids et biais du réseau de neurones et utilise ce gradient pour ajuster progressivement les valeurs des poids et des biais dans la direction de la pente descendante de la fonction de perte.\n",
        "\n",
        "Voici les optimiseurs disponibles dans TensorFlow :\n",
        "- `SGD` (Stochastic Gradient Descent) : l'algorithme de descente de gradient stochastique classique, qui met à jour les poids du modèle à chaque itération en fonction du gradient de la fonction de perte par rapport aux poids.\n",
        "\n",
        "- `Adam` (Adaptive Moment Estimation) : un algorithme d'optimisation adaptatif qui utilise des estimations adaptatives des moments du premier et du second ordre pour mettre à jour les poids du modèle.\n",
        "\n",
        "- `RMSprop` (Root Mean Square Propagation) : un algorithme d'optimisation qui utilise une moyenne mobile du carré des gradients pour normaliser les mises à jour des poids.\n",
        "\n",
        "- `Adagrad` (Adaptive Gradient Algorithm) : un algorithme d'optimisation qui adapte les taux d'apprentissage de chaque poids du modèle en fonction de l'historique des mises à jour.\n",
        "\n",
        "- `Adamax` (Adaptive Moment Estimation with infinity norm) : une variante d'Adam qui utilise la norme infinie pour normaliser les mises à jour des poids.\n",
        "\n",
        "- `Nadam` (Nesterov-accelerated Adaptive Moment Estimation) : une variante d'Adam qui incorpore la méthode Nesterov Accelerated Gradient pour accélérer la convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZqgREzhsczR"
      },
      "source": [
        "----\n",
        "### Exercice 1 : Entrainement d'un MLP sur le jeu de données `Bank marketing`\n",
        "\n",
        "Créez un nouveau notebook intitulé `Modèle DeepLearning - Bank marketing` puis répondez aux questions suivantes. Chaque partie doit être séparée par un titre de niveau 2 et doit être accompagnée d'un commentaire expliquant votre démarche.\n",
        "\n",
        "1. Complétez le code suivant en renseignant le chemin vers le jeu de données `Bank marketing` puis exécutez le code afin d'obtenir les variables d'entrainement et de test.\n",
        "\n",
        "2. Créez un réseau de neuronnes profond avec 3 couches cachées de 10 neurones chacune et une couche de sortie de 1 neurone. Utilisez la fonction d'activation `relu` pour les couches cachées et `sigmoid` pour la couche de sortie. \n",
        "\n",
        "3. Compilez le modèle en utilisant l'optimiseur `adam` et la fonction de perte `binary_crossentropy`.\n",
        "\n",
        "4. Entraînez le modèle en utilisant 10 epochs et une taille de batch de 32 et stockez l'historique de l'entraînement dans la variable `history`.   \n",
        "\n",
        "Utilisez la méthode fit du modèle : `fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "          epochs=5, batch_size=10, verbose=1)`\n",
        "\n",
        "5. Affichez l'historique de l'entraînement en utilisant la fonction `plot_history`.\n",
        "\n",
        "6. Obtenez les prédictions du modèle sur les données de test avec la méthode `predict` et calculez l'accuracy du modèle avec la fonction `accuracy_score` de la librairie `sklearn.metrics`. Utilisez une liste compréhension pour convertir les prédictions en 0 ou 1 avec un seuil de 0.5.\n",
        "\n",
        "7. Affichez les poids du modèle avec l'attribut `get_weights` du modèle. Comment est-ce que les poids sont stockés ?\n",
        "\n",
        "8. Changez les hyperparamètres du modèle et observez l'impact sur l'accuracy du modèle.\n",
        "- fonction d'activation, \n",
        "- nombre de couches, \n",
        "- nombre de neurones, \n",
        "- optimiseur, \n",
        "- fonction de perte, \n",
        "- nombre d'epochs, \n",
        "- taille de batch\n",
        "\n",
        "Au moins 3 configurations différentes doivent être testées.\n",
        "Quel est le meilleur modèle que vous avez obtenu ? \n",
        "\n",
        "9. Sauvegardez le modèle en utilisant la méthode `save` du modèle. Chargez le modèle en utilisant la méthode `load_model` de la librairie `tensorflow.keras.models`.\n",
        "\n",
        "10. Publiez votre projet sur GitHub et placez le lien dans dans le fichier [suivant](https://docs.google.com/spreadsheets/d/1mnS5XL4pJNmQNclndP2LZPCELOi1OmBKoW3803xyK8c/edit?usp=sharing).\n",
        "\n",
        "**Bonus :** Déposez le `Tp 6 : Introduction au Deep Learning` dans votre dossier Github avec vos noms et prénoms dans le nom du fichier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9KByLF2sczR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "def plot_history(history):\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['train', 'val'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "file_path = #Ajouter le chemin vers le fichier\n",
        "\n",
        "\n",
        "# Import du jeu de données\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "X = df.drop(['ID', 'contact', 'month', 'day', 'y'], axis=1)\n",
        "y = df['y']\n",
        "\n",
        "# Encodage des variables catégorielles\n",
        "le = LabelEncoder()\n",
        "X['job'] = le.fit_transform(X['job'])\n",
        "X['marital'] = le.fit_transform(X['marital'])\n",
        "X['education'] = le.fit_transform(X['education'])\n",
        "X['default'] = le.fit_transform(X['default'])\n",
        "X['housing'] = le.fit_transform(X['housing'])\n",
        "X['loan'] = le.fit_transform(X['loan'])\n",
        "X['poutcome'] = le.fit_transform(X['poutcome'])\n",
        "\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Normalisation des données\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "\n",
        "# Séparer les données en train et test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}